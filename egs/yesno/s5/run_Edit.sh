#!/bin/bash



###
## PREPARE FILES FOR TRAINING & DECODING
#

# Download the data if we dont have it already
if [ ! -d waves_yesno ]; then
  wget http://www.openslr.org/resources/1/waves_yesno.tar.gz || exit 1;
  tar -xvzf waves_yesno.tar.gz || exit 1;
fi

# delete the three dirs generated by this script if they already exist
# so that we have a clean slate
rm -rf data exp mfcc

# Given dir of WAV files, create dirs for train and test, create 'wav.scp',
# create 'text', create 'utt2spk' and 'spk2utt', and copy the language model
# from elsewhere (ARPA format) 
local/prepare_data.sh waves_yesno

# Copy and paste existing phonetic dictionary, language model, and phone list
local/prepare_dict.sh

# Prepare (1) a directory such as data/lang/ (2) silence_phones.txt, 
# (3) nonsilence_phones.txt (4) optional_silence.txt and (5) extra_questions.txt
# This script adds word-position-dependent phones and constructs a host of other
# derived files, that go in data/lang/.
# prepare_lang.sh [--options] <dict-src-dir> <oov-dict-entry> <tmp-dir> <lang-dir>
utils/prepare_lang.sh --position-dependent-phones false data/local/dict "<SIL>" data/local/lang data/lang

# I think this script has to do with making a LM a FST
local/prepare_lm.sh

# Feature extraction 
for x in train_yesno test_yesno; do 
 # Get the MFCCs
 # steps/make_mfcc.sh [options] <data-dir> <log-dir> <path-to-mfccdir>
 steps/make_mfcc.sh --nj 1 data/$x exp/make_mfcc/$x mfcc
 # Compute cepstral mean and variance statistics per speaker.
 # steps/compute_cmvn_stats.sh [options] <data-dir> <log-dir> <path-to-cmvn-dir>
 steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc
done



###
## TRAINING
#

# This run.pl script takes our commands and runs them in bash, 
# parallelized if we want, and prints out a log file.
train_cmd="utils/run.pl"
# Flat start and monophone training, with delta-delta features.
# This script applies cepstral mean normalization (per speaker).
# steps/train_mono.sh [options] <data-dir> <lang-dir> <exp-dir>
steps/train_mono.sh --nj 1 --cmd "$train_cmd" \
  --totgauss 400 \
  data/train_yesno data/lang exp/mono0a 
  


###
## DECODING
#

# Graph compilation  
# This script creates a fully expanded decoding graph (HCLG) that represents
# the language-model, pronunciation dictionary (lexicon), context-dependency,
# and HMM structure in our model.  The output is a Finite State Transducer
# that has word-ids on the output, and pdf-ids on the input (these are indexes
# that resolve to Gaussian Mixture Models).
# utils/mkgraph.sh [options] <lang-dir> <model-dir> <graphdir> 
utils/mkgraph.sh --mono data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr

# Test the graph on new files
# This run.pl script takes our commands and runs them in bash, 
# parallelized if we want, and prints out a log file.
decode_cmd="utils/run.pl"
# steps/decode.sh [options] <graph-dir> <data-dir> <decode-dir>
steps/decode.sh --nj 1 --cmd "$decode_cmd" \
    exp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno
# calculate the WER
for x in exp/*/decode*; do 
    [ -d $x ] && grep WER $x/wer_* | utils/best_wer.sh; 
done
